{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Setup\n","Install Confluent-Kafka client"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: confluent-kafka in /Users/kellyliu/anaconda3/lib/python3.11/site-packages (2.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install confluent-kafka\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: confluent-kafka\n","Version: 2.3.0\n","Summary: Confluent's Python client for Apache Kafka\n","Home-page: https://github.com/confluentinc/confluent-kafka-python\n","Author: Confluent Inc\n","Author-email: support@confluent.io\n","License: \n","Location: /Users/kellyliu/anaconda3/lib/python3.11/site-packages\n","Requires: \n","Required-by: \n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip show confluent-kafka"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Batch Processing"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1. Load topic data from Confluent in batch mode"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /Users/kellyliu/anaconda3/lib/python3.11/site-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /Users/kellyliu/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pyspark"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["import os\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"ename":"IndentationError","evalue":"unexpected indent (1625169426.py, line 3)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[66], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    .builder\\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"source":["spark = SparkSession\\\n","\n","  .builder\\\n","\n","  .config(\"spark.jars\", os.getcwd() + \"/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar\" + \",\" + os.getcwd() + \"/jars/kafka-clients-2.1.1.jar\") \\\n","\n","  .appName(\"Structured_Redpanda_WordCount\")\\\n","\n","  .getOrCreate()\n","\n","spark.conf.set(\"spark.sql.shuffle.partitions\", 1)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"ename":"AnalysisException","evalue":"Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[65], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m topic_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m bootstrap_servers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpkc-4rn2p.canadacentral.azure.confluent.cloud:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m df_kafka \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, bootstrap_servers)\\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, topic_name)\\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.security.protocol\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSASL_SSL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.sasl.mechanism\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.sasl.jaas.config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mDMEFD5LIKVBOJCWD\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m password=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mmUEgMQvRZHnL4ZqtkTUww6Dywxxr9HoDmmgzYW6jRGumni3FrstD3c0ERxplHc8j\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     27\u001b[0m display(df_kafka)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."]}],"source":["# from pyspark.sql import SparkSession\n","\n","# spark = SparkSession.builder \\\n","#     .appName(\"KafkaIntegrationExample\") \\\n","#     .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n","#     .getOrCreate()\n","\n","from pyspark.sql import SparkSession\n","from pyspark import SparkConf\n","\n","conf = SparkConf() \\\n","    .setAppName(\"KafkaIntegration\") \\\n","    .set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n","\n","spark = SparkSession.builder.config(conf=conf).getOrCreate()\n","\n","topic_name = \"topic_0\"\n","bootstrap_servers = \"pkc-4rn2p.canadacentral.azure.confluent.cloud:9092\"\n","df_kafka = spark.read.format(\"kafka\")\\\n","    .option(\"kafka.bootstrap.servers\", bootstrap_servers)\\\n","    .option(\"subscribe\", topic_name)\\\n","    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n","    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n","    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"DMEFD5LIKVBOJCWD\\\" password=\\\"mUEgMQvRZHnL4ZqtkTUww6Dywxxr9HoDmmgzYW6jRGumni3FrstD3c0ERxplHc8j\\\";\")\\\n","    .load()\n","\n","display(df_kafka)"]},{"cell_type":"markdown","metadata":{},"source":["\n","## 2.2. Write a Kafka sink for batch queries"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2.1. Create sample data"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# import pyspark class Row from module sql\n","from pyspark.sql import *"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Row(key='1', value='{\"name\": \"Jane Doe\", \"age\": 29, \"email\": \"jane.doe@gmail.com\"}', topic='test-topic')\n"]}],"source":["# Create the sample data\n","sample_data = Row(\"key\", \"value\", \"topic\")\n","sample1 = sample_data('1', '{\"name\": \"Jane Doe\", \"age\": 29, \"email\": \"jane.doe@gmail.com\"}', \"test-topic\")\n","sample2 = sample_data('2', '{\"name\": \"Fatih\", \"age\": 24, \"email\": \"f.nayebi@gmail.com\"}', \"test-topic\")\n","sample3 = sample_data('3', '{\"name\": \"John Doe\", \"age\": 35, \"email\": \"john.doe@gmail.com\"}', \"test-topic\")\n","\n","print(sample1)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2.2. Create a dataframe from sample data"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["df = spark.createDataFrame([sample1, sample2, sample3])"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---+--------------------+----------+\n","|key|               value|     topic|\n","+---+--------------------+----------+\n","|  1|{\"name\": \"Jane Do...|test-topic|\n","|  2|{\"name\": \"Fatih\",...|test-topic|\n","|  3|{\"name\": \"John Do...|test-topic|\n","+---+--------------------+----------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[key: string, value: string, topic: string]"]},"metadata":{},"output_type":"display_data"}],"source":["display(df)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2.3. Write data from a dataframe to a confluent kafka topic"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"ename":"AnalysisException","evalue":"Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ds \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, bootstrap_servers)\\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, topic_name)\\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.security.protocol\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSASL_SSL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.sasl.mechanism\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.sasl.jaas.config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mDMEFD5LIKVBOJCWD\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m password=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mmUEgMQvRZHnL4ZqtkTUww6Dywxxr9HoDmmgzYW6jRGumni3FrstD3c0ERxplHc8j\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m, topic_name)\\\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."]}],"source":["# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n","ds = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n","    .write.format(\"kafka\")\\\n","    .option(\"kafka.bootstrap.servers\", bootstrap_servers)\\\n","    .option(\"subscribe\", topic_name)\\\n","    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n","    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n","    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"DMEFD5LIKVBOJCWD\\\" password=\\\"mUEgMQvRZHnL4ZqtkTUww6Dywxxr9HoDmmgzYW6jRGumni3FrstD3c0ERxplHc8j\\\";\")\\\n","    .option(\"topic\", topic_name)\\\n","    .save()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Stream Processing"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1. Read a stream from Kafka"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th><th>topic</th><th>partition</th><th>offset</th><th>timestamp</th><th>timestampType</th></tr></thead><tbody></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["df_stream = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", bootstrap.servers)\\\n","    .option(\"subscribe\", topic_name)\\\n","    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n","    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n","    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Replace with your api key\\\" password=\\\"replace with your api secret\\\";\")\\\n","    .load() \\\n","\n","display(df_stream)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2. Write a Kafka sink for streaming queries"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n","ds = df_stream \\\n","  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n","  .writeStream \\\n","  .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", bootstrap.servers)\\\n","    .option(\"subscribe\", topic_name)\\\n","    .option(\"kafka.security.protocol\",\"SASL_SSL\")\\\n","    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n","    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Replace with your api key\\\" password=\\\"replace with your api secret\\\";\")\\\n","  .option(\"topic\", \"databricks_test\") \\\n","  .option(\"checkpointLocation\", \"/dbfs/dir\") \\\n","  .start()"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"name":"Kafka-spark","notebookId":250246357806386},"nbformat":4,"nbformat_minor":0}
